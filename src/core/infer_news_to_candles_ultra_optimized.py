#!/usr/bin/env python3
"""
–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
"""
import torch
import torch.nn as nn
import numpy as np
from typing import Dict, List, Tuple
import pandas as pd
from functools import lru_cache
import os
import json
import math

from src.ml.nn_model import NewsTickerModel
from src.ml.nn_data import load_vocab, encode_text


class UltraOptimizedNewsTickerModel(nn.Module):
    """–£–ª—å—Ç—Ä–∞-–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ —É–ª—É—á—à–µ–Ω–∏—è–º–∏"""
    def __init__(self, vocab_size: int, num_labels: int, embed_dim: int = 256, rnn_hidden: int = 256, dropout: float = 0.1):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–æ–ª–µ–µ –±—ã—Å—Ç—Ä—ã–π GRU —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
        self.bigru = nn.GRU(embed_dim, rnn_hidden, num_layers=1, batch_first=True, bidirectional=True)
        self.dropout = nn.Dropout(dropout)
        self.pool = nn.Linear(rnn_hidden * 2, 1)  # –£–ø—Ä–æ—â–µ–Ω–Ω—ã–π pooling
        self.classifier = nn.Sequential(
            nn.Linear(rnn_hidden * 2, num_labels),
        )

    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:
        x = self.embedding(input_ids)
        x = self.dropout(x)
        h, _ = self.bigru(x)
        
        # –£–ø—Ä–æ—â–µ–Ω–Ω—ã–π pooling –±–µ–∑ attention
        pooled = torch.mean(h, dim=1)
        logits = self.classifier(pooled)
        return logits


class ModelCache:
    """–ö—ç—à –¥–ª—è –º–æ–¥–µ–ª–µ–π —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º –ø–∞–º—è—Ç—å—é"""
    def __init__(self, max_size: int = 3):
        self.cache = {}
        self.max_size = max_size
        self.access_order = []
    
    def get(self, key: str):
        if key in self.cache:
            # –û–±–Ω–æ–≤–ª—è–µ–º –ø–æ—Ä—è–¥–æ–∫ –¥–æ—Å—Ç—É–ø–∞
            self.access_order.remove(key)
            self.access_order.append(key)
            return self.cache[key]
        return None
    
    def put(self, key: str, value):
        if len(self.cache) >= self.max_size:
            # –£–¥–∞–ª—è–µ–º –Ω–∞–∏–º–µ–Ω–µ–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–π —ç–ª–µ–º–µ–Ω—Ç
            oldest = self.access_order.pop(0)
            del self.cache[oldest]
        
        self.cache[key] = value
        self.access_order.append(key)


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π –∫—ç—à –º–æ–¥–µ–ª–µ–π
_model_cache = ModelCache()
_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')


@lru_cache(maxsize=2)
def load_artifacts_ultra_cached(artifacts: str):
    """–£–ª—å—Ç—Ä–∞-–∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤"""
    cache_key = f"artifacts_{artifacts}"
    cached = _model_cache.get(cache_key)
    if cached is not None:
        return cached
    
    with open(os.path.join(artifacts, 'tickers.json'), 'r', encoding='utf-8') as f:
        ticker_to_idx = json.load(f)['ticker_to_idx']
    vocab = load_vocab(os.path.join(artifacts, 'vocab.json'))
    ckpt = torch.load(os.path.join(artifacts, 'model.pt'), map_location='cpu')
    
    # –°–æ–∑–¥–∞–µ–º —É–ª—å—Ç—Ä–∞-–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å
    model = UltraOptimizedNewsTickerModel(vocab_size=len(vocab), num_labels=len(ticker_to_idx))
    model.load_state_dict(ckpt['state_dict'])
    model.to(_device)
    model.eval()
    
    # –ö–æ–º–ø–∏–ª–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å –¥–ª—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ —É—Å–∫–æ—Ä–µ–Ω–∏—è (PyTorch 2.0+)
    try:
        model = torch.compile(model)
    except:
        pass  # –ï—Å–ª–∏ –∫–æ–º–ø–∏–ª—è—Ü–∏—è –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è
    
    result = {
        'ticker_to_idx': ticker_to_idx,
        'vocab': vocab,
        'model': model,
        'config': ckpt.get('config', {})
    }
    
    _model_cache.put(cache_key, result)
    return result


def score_news_ultra_optimized(df_news: pd.DataFrame, artifacts_data: dict, batch_size: int = 1024) -> np.ndarray:
    """–£–ª—å—Ç—Ä–∞-–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Å–∫–æ—Ä–∏–Ω–≥–∞"""
    model = artifacts_data['model']
    vocab = artifacts_data['vocab']
    max_len = artifacts_data['config'].get('max_len', 256)
    
    scores = []
    with torch.no_grad():
        # –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º –≤—Å–µ —Ç–µ–∫—Å—Ç—ã
        all_texts = (df_news['title'].fillna('') + ' [SEP] ' + df_news['publication'].fillna('')).tolist()
        all_ids = [encode_text(t, vocab, max_len) for t in all_texts]
        
        for i in range(0, len(all_ids), batch_size):
            batch_ids = all_ids[i:i+batch_size]
            maxL = max(len(x) for x in batch_ids) if batch_ids else 1
            
            # –°–æ–∑–¥–∞–µ–º —Ç–µ–Ω–∑–æ—Ä—ã –Ω–∞ GPU —Å—Ä–∞–∑—É
            batch_size_actual = len(batch_ids)
            input_ids = torch.zeros((batch_size_actual, maxL), dtype=torch.long, device=_device)
            attention_mask = torch.zeros((batch_size_actual, maxL), dtype=torch.long, device=_device)
            
            for j, seq in enumerate(batch_ids):
                L = len(seq)
                input_ids[j, :L] = torch.tensor(seq, dtype=torch.long, device=_device)
                attention_mask[j, :L] = 1
            
            logits = model(input_ids, attention_mask)
            scores.append(torch.sigmoid(logits).cpu().numpy())
    
    return np.vstack(scores) if scores else np.zeros((0, len(artifacts_data['ticker_to_idx'])))


def aggregate_to_candles_ultra_optimized(
    df_candles: pd.DataFrame,
    df_news: pd.DataFrame,
    scores: np.ndarray,
    ticker_to_idx: dict,
    half_life_days: float = 2.0,
    p_threshold: float = 0.5,
    max_days: float = 20.0,
) -> pd.DataFrame:
    """–£–ª—å—Ç—Ä–∞-–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∞–≥—Ä–µ–≥–∞—Ü–∏—è —Å –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–µ–π"""
    scores = np.asarray(scores)
    df_news = df_news.copy()
    df_news['publish_date'] = pd.to_datetime(df_news['publish_date'], errors='coerce')
    df_news['date'] = df_news['publish_date'].dt.date

    df_candles = df_candles.copy()
    df_candles['begin'] = pd.to_datetime(df_candles['begin'], errors='coerce')
    df_candles['date'] = df_candles['begin'].dt.date

    news_dates = pd.to_datetime(df_news['date']).values.astype('datetime64[D]')
    decay_lambda = math.log(2) / max(half_life_days, 1e-6)
    max_days = float(max_days) if max_days is not None else np.inf

    features = []
    valid_tickers = set(ticker_to_idx.keys())
    
    # –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –≤—ã—á–∏—Å–ª—è–µ–º –≤—Å–µ –º–∞—Å–∫–∏ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
    candle_dates = df_candles['date'].unique()
    candle_dates_sorted = np.sort(candle_dates)
    
    for ticker, g in df_candles.groupby('ticker'):
        if ticker not in valid_tickers:
            continue
            
        label_idx = ticker_to_idx[ticker]
        probs = np.asarray(scores[:, label_idx], dtype=np.float64)
        
        for dt, sub in g.groupby('date'):
            dt64 = np.datetime64(dt)
            
            # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –æ–∫–Ω–æ –≤—Ä–µ–º–µ–Ω–∏
            if np.isfinite(max_days):
                min_dt64 = dt64 - np.timedelta64(int(max_days), 'D')
                base_mask = (news_dates <= dt64) & (news_dates >= min_dt64)
            else:
                base_mask = (news_dates <= dt64)
                
            if not base_mask.any():
                features.append({
                    'ticker': ticker, 'date': dt, 
                    'nn_news_sum': 0.0, 'nn_news_mean': 0.0, 
                    'nn_news_max': 0.0, 'nn_news_count': 0
                })
                continue
                
            # –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –ø–æ –ø–æ—Ä–æ–≥—É
            mask_thr = base_mask & (probs >= p_threshold)
            if not mask_thr.any():
                features.append({
                    'ticker': ticker, 'date': dt, 
                    'nn_news_sum': 0.0, 'nn_news_mean': 0.0, 
                    'nn_news_max': 0.0, 'nn_news_count': 0
                })
                continue
                
            # –í–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω–æ–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ –∑–∞—Ç—É—Ö–∞–Ω–∏—è
            deltas = (dt64 - news_dates[mask_thr]).astype('timedelta64[D]').astype(int)
            weights = np.exp(-decay_lambda * deltas.astype(np.float64))
            vals = probs[mask_thr]
            wvals = vals * weights
            
            cnt = int(mask_thr.sum())
            ssum = float(np.sum(wvals))
            smean = float(np.mean(wvals)) if cnt > 0 else 0.0
            smax = float(np.max(wvals)) if cnt > 0 else 0.0
            
            features.append({
                'ticker': ticker, 'date': dt, 
                'nn_news_sum': ssum, 'nn_news_mean': smean, 
                'nn_news_max': smax, 'nn_news_count': cnt
            })
    
    return pd.DataFrame(features)


def infer_news_to_candles_df_ultra_optimized(news_df: pd.DataFrame, candles_df: pd.DataFrame, artifacts_dir: str, 
                                            p_threshold: float = 0.5, half_life_days: float = 0.5, max_days: int = 5) -> tuple:
    """–£–ª—å—Ç—Ä–∞-–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –æ—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    artifacts_data = load_artifacts_ultra_cached(artifacts_dir)
    
    scores = score_news_ultra_optimized(news_df, artifacts_data)
    
    features_df = aggregate_to_candles_ultra_optimized(
        candles_df, news_df, scores, artifacts_data['ticker_to_idx'],
        half_life_days=half_life_days,
        p_threshold=p_threshold,
        max_days=max_days,
    )
    
    # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ
    candles_df_copy = candles_df.copy()
    candles_df_copy['date'] = pd.to_datetime(candles_df_copy['begin'], errors='coerce').dt.date
    features_df_copy = features_df.copy()
    
    joined_df = candles_df_copy.merge(features_df_copy, on=['ticker', 'date'], how='left')
    
    # –í–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–Ω–æ–µ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–æ–ø—É—Å–∫–æ–≤
    feature_cols = ['nn_news_sum', 'nn_news_mean', 'nn_news_max', 'nn_news_count']
    for col in feature_cols:
        if col in joined_df.columns:
            joined_df[col] = joined_df[col].fillna(0.0)
    
    return features_df, joined_df


if __name__ == "__main__":
    # –¢–µ—Å—Ç —É–ª—å—Ç—Ä–∞-–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –≤–µ—Ä—Å–∏–∏
    import time
    import sys
    from pathlib import Path
    
    sys.path.insert(0, str(Path(__file__).parent / 'src'))
    
    # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
    np.random.seed(42)
    news_data = []
    publications = ['–†–ë–ö', '–ö–æ–º–º–µ—Ä—Å–∞–Ω—Ç', '–í–µ–¥–æ–º–æ—Å—Ç–∏']
    titles = ['–°–±–µ—Ä–±–∞–Ω–∫ –æ–±—ä—è–≤–∏–ª –æ —Ä–æ—Å—Ç–µ –ø—Ä–∏–±—ã–ª–∏', '–ì–∞–∑–ø—Ä–æ–º —É–≤–µ–ª–∏—á–∏–ª –¥–æ–±—ã—á—É –≥–∞–∑–∞']
    
    for i in range(500):
        news_data.append({
            'publish_date': f'2024-01-{(i % 30) + 1:02d}',
            'title': np.random.choice(titles),
            'publication': np.random.choice(publications)
        })
    
    candles_data = []
    tickers = ['SBER', 'GAZP', 'LKOH']
    
    for i in range(250):
        candles_data.append({
            'begin': f'2024-01-{(i % 30) + 1:02d} 10:00:00',
            'ticker': np.random.choice(tickers),
            'open': 100 + np.random.randn() * 10,
            'high': 110 + np.random.randn() * 10,
            'low': 90 + np.random.randn() * 10,
            'close': 105 + np.random.randn() * 10,
            'volume': np.random.randint(1000, 10000)
        })
    
    df_news = pd.DataFrame(news_data)
    df_candles = pd.DataFrame(candles_data)
    
    print("üöÄ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —É–ª—å—Ç—Ä–∞-–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –≤–µ—Ä—Å–∏–∏...")
    start_time = time.time()
    
    try:
        features_df, joined_df = infer_news_to_candles_df_ultra_optimized(
            df_news, df_candles, "artifacts",
            p_threshold=0.5, half_life_days=2.0, max_days=10.0
        )
        
        end_time = time.time()
        print(f"‚ö° –£–ª—å—Ç—Ä–∞-–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è: {end_time - start_time:.2f} —Å–µ–∫—É–Ω–¥")
        print(f"üìä –†–µ–∑—É–ª—å—Ç–∞—Ç: {len(features_df)} —Ñ–∏—á–µ–π, {len(joined_df)} –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã—Ö —Å—Ç—Ä–æ–∫")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
